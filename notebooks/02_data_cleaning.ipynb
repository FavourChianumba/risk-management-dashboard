{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Management Dashboard - Data Cleaning & Preprocessing\n",
    "\n",
    "This notebook handles the cleaning and preprocessing of market and macroeconomic data for the risk management dashboard. It builds on the data retrieved in the previous notebook (`01_data_retrieval.ipynb`).\n",
    "\n",
    "## Overview of Steps\n",
    "\n",
    "1. Load raw data from data retrieval stage\n",
    "2. Inspect data quality (missing values, outliers, etc.)\n",
    "3. Clean and preprocess market data\n",
    "4. Clean and preprocess macroeconomic data\n",
    "5. Align datasets to common dates\n",
    "6. Calculate returns\n",
    "7. Create portfolio based on asset weights\n",
    "8. Analyze return distributions and correlations\n",
    "9. Save processed data for use in risk models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# Configure plot styles\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "sns.set_palette('muted')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up project paths\n",
    "PROJECT_ROOT = Path().resolve().parents[0]\n",
    "CONFIG_DIR = PROJECT_ROOT / \"configs\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Make sure processed data directory exists\n",
    "PROCESSED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data directory: {RAW_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Data Processing Module and Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the path\n",
    "import sys\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Import data processing functions\n",
    "from src.data.process_data import (\n",
    "    load_data_config,\n",
    "    clean_data,\n",
    "    calculate_returns,\n",
    "    create_portfolio,\n",
    "    align_data,\n",
    "    process_data_pipeline\n",
    ")\n",
    "\n",
    "# Load data processing configuration\n",
    "config = load_data_config()\n",
    "\n",
    "# Display configuration\n",
    "print(\"Data Processing Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data from Data Retrieval Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the latest file in a directory matching a pattern\n",
    "def get_latest_file(directory, pattern):\n",
    "    files = list(directory.glob(pattern))\n",
    "    if not files:\n",
    "        return None\n",
    "    return max(files, key=lambda f: f.stat().st_mtime)\n",
    "\n",
    "# Load market data - use a more specific pattern to get only Yahoo data\n",
    "market_file = get_latest_file(RAW_DIR, \"yahoo_data_*.csv\")\n",
    "\n",
    "# If no Yahoo data file found, try other patterns\n",
    "if not market_file:\n",
    "    market_file = get_latest_file(RAW_DIR, \"market_data_*.csv\")\n",
    "    \n",
    "if not market_file:\n",
    "    print(\"No market data files found matching expected patterns.\")\n",
    "    print(\"Available files in RAW_DIR:\")\n",
    "    for file in RAW_DIR.glob(\"*.csv\"):\n",
    "        print(f\"  - {file.name}\")\n",
    "    \n",
    "    # Fall back to choosing from available files\n",
    "    all_files = list(RAW_DIR.glob(\"*.csv\"))\n",
    "    if all_files:\n",
    "        print(\"\\nAttempting to identify market data from available files...\")\n",
    "        for file in all_files:\n",
    "            if \"fred\" not in file.name.lower():  # Skip FRED files\n",
    "                market_file = file\n",
    "                print(f\"Selected {market_file} as potential market data file\")\n",
    "                break\n",
    "\n",
    "if market_file:\n",
    "    print(f\"Loading market data from {market_file}\")\n",
    "    market_data = pd.read_csv(market_file, index_col=0, parse_dates=True)\n",
    "    # Check if we need to convert to MultiIndex\n",
    "    if len(market_data.columns) > 0 and '_' in market_data.columns[0]:\n",
    "        # This is likely a flattened MultiIndex\n",
    "        try:\n",
    "            market_data.columns = pd.MultiIndex.from_tuples(\n",
    "                [tuple(col.split('_', 1)) for col in market_data.columns]\n",
    "            )\n",
    "            print(\"Converted columns to MultiIndex format\")\n",
    "        except:\n",
    "            print(\"Could not convert columns to MultiIndex format\")\n",
    "    \n",
    "    print(f\"Market data shape: {market_data.shape}\")\n",
    "else:\n",
    "    print(\"No market data files found. Please run data retrieval first.\")\n",
    "    market_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When identifying which data is market vs. macro:\n",
    "is_market_data = False\n",
    "if isinstance(market_data.columns, pd.MultiIndex):\n",
    "    # Check if typical market data columns exist\n",
    "    is_market_data = any(col in market_data.columns.levels[0] \n",
    "                         for col in ['TRDPRC_1', 'Close', 'Adj Close'])\n",
    "else:\n",
    "    # Check if market tickers exist\n",
    "    is_market_data = any(ticker in market_data.columns \n",
    "                        for ticker in ['SPX', 'US10YT=RR', '^GSPC', '^TNX'])\n",
    "\n",
    "if not is_market_data:\n",
    "    print(\"WARNING: Your market_data appears to contain macro indicators instead of market prices!\")\n",
    "    print(\"Please check your data retrieval process to ensure market data is being loaded correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading market_data\n",
    "print(\"Market data columns:\")\n",
    "if isinstance(market_data.columns, pd.MultiIndex):\n",
    "    print(f\"Level 0: {market_data.columns.levels[0].tolist()}\")\n",
    "    print(f\"Level 1: {market_data.columns.levels[1].tolist()}\")\n",
    "else:\n",
    "    print(market_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load macroeconomic data\n",
    "macro_file = get_latest_file(RAW_DIR, \"fred_data_*.csv\")\n",
    "\n",
    "if macro_file:\n",
    "    print(f\"Loading macroeconomic data from {macro_file}\")\n",
    "    macro_data = pd.read_csv(macro_file, index_col=0, parse_dates=True)\n",
    "    print(f\"Macroeconomic data shape: {macro_data.shape}\")\n",
    "else:\n",
    "    print(\"No macroeconomic data files found. Continuing without macro data.\")\n",
    "    macro_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if market_file:\n",
    "    print(f\"Loading market data from {market_file}\")\n",
    "    \n",
    "    # First, check file format by looking at the first few lines\n",
    "    with open(market_file, 'r') as f:\n",
    "        first_few_lines = [f.readline() for _ in range(5)]\n",
    "    \n",
    "    print(\"File preview:\")\n",
    "    for line in first_few_lines:\n",
    "        print(line.strip())\n",
    "    \n",
    "    # Try loading with proper date index\n",
    "    try:\n",
    "        # First row is column headers, second row has asset names\n",
    "        # Date column is actually in the third row\n",
    "        market_data = pd.read_csv(market_file, header=[0, 1], index_col=0, parse_dates=True)\n",
    "        print(\"Loaded with multi-level headers\")\n",
    "        \n",
    "        # Clean up any 'Date' in the index\n",
    "        if 'Date' in market_data.index:\n",
    "            market_data = market_data.drop('Date')\n",
    "            print(\"Dropped 'Date' from index\")\n",
    "    except Exception as e:\n",
    "        print(f\"Multi-level header loading failed: {e}\")\n",
    "        \n",
    "        try:\n",
    "            # Fallback: Load with skiprows to handle the date issue\n",
    "            market_data = pd.read_csv(market_file, skiprows=2, index_col=0, parse_dates=True)\n",
    "            print(\"Loaded with skiprows=2\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skiprows loading failed: {e}\")\n",
    "            market_data = pd.DataFrame()\n",
    "    \n",
    "    # Ensure all data is numeric\n",
    "    for col in market_data.columns:\n",
    "        try:\n",
    "            market_data[col] = pd.to_numeric(market_data[col], errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"Market data shape: {market_data.shape}\")\n",
    "else:\n",
    "    print(\"No market data files found. Please run data retrieval first.\")\n",
    "    market_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze data quality\n",
    "def analyze_data_quality(df, title=\"Dataset\"):\n",
    "    print(f\"\\n=== {title} Quality Analysis ===\")\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"Empty DataFrame, skipping analysis\")\n",
    "        return\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    # Check if index is datetime\n",
    "    is_datetime_index = pd.api.types.is_datetime64_any_dtype(df.index)\n",
    "    \n",
    "    if is_datetime_index:\n",
    "        # If we have a proper datetime index\n",
    "        print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "        print(f\"Total days: {len(df)}\")\n",
    "        \n",
    "        # Calendar coverage\n",
    "        date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "        missing_days = len(date_range) - len(df)\n",
    "        print(f\"Calendar days in range: {len(date_range)}\")\n",
    "        print(f\"Missing days: {missing_days} ({missing_days / len(date_range):.2%})\")\n",
    "    else:\n",
    "        # For non-datetime index\n",
    "        print(f\"Index type: {df.index.dtype}\")\n",
    "        print(f\"Index range: {df.index.min()} to {df.index.max()}\")\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        print(\"Note: Index is not datetime type, calendar coverage analysis skipped\")\n",
    "    \n",
    "    # Missing values\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_cells = df.isna().sum().sum()\n",
    "    print(f\"Missing values: {missing_cells} ({missing_cells / total_cells:.2%})\")\n",
    "        \n",
    "    # Missing values by column\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # For MultiIndex columns, we'll check by level 0 (data type) and level 1 (asset)\n",
    "        print(\"\\nMissing values by data type:\")\n",
    "        for level0 in df.columns.levels[0]:\n",
    "            sub_df = df[level0]\n",
    "            missing = sub_df.isna().sum().sum()\n",
    "            total = sub_df.shape[0] * sub_df.shape[1]\n",
    "            print(f\"  - {level0}: {missing} ({missing / total:.2%})\")\n",
    "        \n",
    "        print(\"\\nMissing values by asset (using Close/TRDPRC_1):\")\n",
    "        # Determine which price column to use\n",
    "        price_col = None\n",
    "        for col in ['TRDPRC_1', 'Close', 'Adj Close']:\n",
    "            if col in df.columns.levels[0]:\n",
    "                price_col = col\n",
    "                break\n",
    "        \n",
    "        if price_col:\n",
    "            for asset in df[price_col].columns:\n",
    "                missing = df[price_col][asset].isna().sum()\n",
    "                total = len(df)\n",
    "                print(f\"  - {asset}: {missing} ({missing / total:.2%})\")\n",
    "    else:\n",
    "        print(\"\\nMissing values by column:\")\n",
    "        missing_by_col = df.isna().sum()\n",
    "        for col, missing in missing_by_col.items():\n",
    "            if missing > 0:\n",
    "                print(f\"  - {col}: {missing} ({missing / len(df):.2%})\")\n",
    "    \n",
    "    # Potential outliers\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # For price data, we'll look at daily returns for outliers\n",
    "        if price_col:\n",
    "            print(\"\\nPotential outliers in daily price changes:\")\n",
    "            for asset in df[price_col].columns:\n",
    "                returns = df[price_col][asset].pct_change().dropna()\n",
    "                # Identify outliers as returns beyond 3 standard deviations\n",
    "                mean = returns.mean()\n",
    "                std = returns.std()\n",
    "                outliers = returns[abs(returns - mean) > 3 * std]\n",
    "                if not outliers.empty:\n",
    "                    print(f\"  - {asset}: {len(outliers)} outliers ({len(outliers) / len(returns):.2%})\")\n",
    "                    # Show the top 5 largest outliers\n",
    "                    top_outliers = outliers.abs().nlargest(5).index\n",
    "                    print(f\"    Top outliers: {', '.join([f'{date.date()}: {returns[date]:.2%}' for date in top_outliers])}\")\n",
    "    else:\n",
    "        # For numeric columns, detect outliers\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(\"\\nPotential outliers in numeric columns:\")\n",
    "            for col in numeric_cols:\n",
    "                values = df[col].dropna()\n",
    "                # Calculate z-scores\n",
    "                z_scores = np.abs((values - values.mean()) / values.std())\n",
    "                outliers = z_scores[z_scores > 3]\n",
    "                if len(outliers) > 0:\n",
    "                    print(f\"  - {col}: {len(outliers)} outliers ({len(outliers) / len(values):.2%})\")\n",
    "\n",
    "# Analyze market data quality\n",
    "analyze_data_quality(market_data, \"Market Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze macroeconomic data quality\n",
    "analyze_data_quality(macro_data, \"Macroeconomic Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot time series data\n",
    "def plot_time_series(df, title=\"Time Series Data\", cols=None, figsize=(14, 8)):\n",
    "    if df.empty:\n",
    "        print(f\"Empty DataFrame, skipping {title} plot\")\n",
    "        return\n",
    "    \n",
    "    # Ensure data is numeric\n",
    "    numeric_df = df.copy()\n",
    "    for col in numeric_df.columns:\n",
    "        numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce')\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Handle MultiIndex DataFrame\n",
    "    if isinstance(numeric_df.columns, pd.MultiIndex):\n",
    "        # Determine which price column to use\n",
    "        price_col = None\n",
    "        for col in ['TRDPRC_1', 'Close', 'Adj Close']:\n",
    "            if col in numeric_df.columns.levels[0]:\n",
    "                price_col = col\n",
    "                break\n",
    "        \n",
    "        if price_col:\n",
    "            try:\n",
    "                # Normalize to 100 at the start\n",
    "                first_row = numeric_df[price_col].iloc[0]\n",
    "                if (first_row == 0).any() or first_row.isna().any():\n",
    "                    # Find first non-zero, non-NaN row\n",
    "                    for i in range(1, len(numeric_df)):\n",
    "                        if not (numeric_df[price_col].iloc[i] == 0).any() and not numeric_df[price_col].iloc[i].isna().any():\n",
    "                            first_row = numeric_df[price_col].iloc[i]\n",
    "                            break\n",
    "                \n",
    "                normalized = numeric_df[price_col].div(first_row) * 100\n",
    "                normalized.plot(ax=plt.gca())\n",
    "                plt.title(f\"{title} - Normalized to 100\")\n",
    "                plt.ylabel(\"Normalized Value\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error normalizing data: {e}\")\n",
    "                # Fallback: just plot the raw data\n",
    "                numeric_df[price_col].plot(ax=plt.gca())\n",
    "                plt.title(f\"{title} - Raw Values\")\n",
    "                plt.ylabel(\"Value\")\n",
    "        else:\n",
    "            print(f\"No suitable price columns found in {title}\")\n",
    "            return\n",
    "    else:\n",
    "        # For regular DataFrame\n",
    "        if cols is None:\n",
    "            cols = numeric_df.columns\n",
    "        \n",
    "        try:\n",
    "            # Normalize to 100 at the start\n",
    "            first_row = numeric_df[cols].iloc[0]\n",
    "            if (first_row == 0).any() or first_row.isna().any():\n",
    "                # Find first non-zero, non-NaN row\n",
    "                for i in range(1, len(numeric_df)):\n",
    "                    if not (numeric_df[cols].iloc[i] == 0).any() and not numeric_df[cols].iloc[i].isna().any():\n",
    "                        first_row = numeric_df[cols].iloc[i]\n",
    "                        break\n",
    "            \n",
    "            normalized = numeric_df[cols].div(first_row) * 100\n",
    "            normalized.plot(ax=plt.gca())\n",
    "            plt.title(f\"{title} - Normalized to 100\")\n",
    "            plt.ylabel(\"Normalized Value\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error normalizing data: {e}\")\n",
    "            # Fallback: just plot the raw data\n",
    "            numeric_df[cols].plot(ax=plt.gca())\n",
    "            plt.title(f\"{title} - Raw Values\")\n",
    "            plt.ylabel(\"Value\")\n",
    "    \n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot market data\n",
    "plot_time_series(market_data, \"Market Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot macro data\n",
    "plot_time_series(macro_data, \"Macroeconomic Indicators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values in market data\n",
    "def plot_missing_values(df, title=\"Missing Values\"):\n",
    "    if df.empty:\n",
    "        print(f\"Empty DataFrame, skipping {title} plot\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Handle MultiIndex DataFrame\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Determine which price column to use\n",
    "        price_col = None\n",
    "        for col in ['TRDPRC_1', 'Close', 'Adj Close']:\n",
    "            if col in df.columns.levels[0]:\n",
    "                price_col = col\n",
    "                break\n",
    "        \n",
    "        if price_col:\n",
    "            # Focus on price data\n",
    "            missing_data = df[price_col].isna()\n",
    "            \n",
    "            # Create a heatmap of missing values\n",
    "            sns.heatmap(missing_data, cmap='viridis', cbar_kws={'label': 'Missing'})\n",
    "            plt.title(f\"Missing Values in {title} - {price_col}\")\n",
    "            plt.xlabel(\"Asset\")\n",
    "            plt.ylabel(\"Date\")\n",
    "        else:\n",
    "            print(f\"No suitable price columns found in {title}\")\n",
    "            return\n",
    "    else:\n",
    "        # For regular DataFrame\n",
    "        missing_data = df.isna()\n",
    "        \n",
    "        # Create a heatmap of missing values\n",
    "        sns.heatmap(missing_data, cmap='viridis', cbar_kws={'label': 'Missing'})\n",
    "        plt.title(f\"Missing Values in {title}\")\n",
    "        plt.xlabel(\"Column\")\n",
    "        plt.ylabel(\"Date\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot missing values in market data\n",
    "plot_missing_values(market_data, \"Market Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clean and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean market data\n",
    "print(\"Cleaning market data...\")\n",
    "cleaned_market = clean_data(market_data, config=config)\n",
    "print(f\"Original shape: {market_data.shape}, Cleaned shape: {cleaned_market.shape}\")\n",
    "\n",
    "# Clean macro data\n",
    "if not macro_data.empty:\n",
    "    print(\"\\nCleaning macroeconomic data...\")\n",
    "    cleaned_macro = clean_data(macro_data, config=config)\n",
    "    print(f\"Original shape: {macro_data.shape}, Cleaned shape: {cleaned_macro.shape}\")\n",
    "else:\n",
    "    cleaned_macro = macro_data\n",
    "    print(\"\\nSkipping macroeconomic data cleaning (no data available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align market and macro data to common dates\n",
    "if not cleaned_macro.empty:\n",
    "    print(\"Aligning market and macro data to common dates...\")\n",
    "    aligned_market, aligned_macro = align_data(cleaned_market, cleaned_macro)\n",
    "    print(f\"Aligned data shapes: Market {aligned_market.shape}, Macro {aligned_macro.shape}\")\n",
    "else:\n",
    "    aligned_market = cleaned_market\n",
    "    aligned_macro = cleaned_macro\n",
    "    print(\"Skipping data alignment (no macro data available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate market returns\n",
    "returns_method = config.get('returns_method', 'log')\n",
    "print(f\"Calculating {returns_method} returns for market data...\")\n",
    "market_returns = calculate_returns(aligned_market, method=returns_method)\n",
    "print(f\"Returns shape: {market_returns.shape}\")\n",
    "\n",
    "# Display return statistics\n",
    "print(\"\\nReturn statistics:\")\n",
    "if isinstance(market_returns.columns, pd.MultiIndex):\n",
    "    # If we have MultiIndex columns, we need to handle them differently\n",
    "    for asset in market_returns.columns.levels[1]:\n",
    "        asset_returns = market_returns.xs(asset, axis=1, level=1)\n",
    "        if not asset_returns.empty:\n",
    "            print(f\"  - {asset}:\")\n",
    "            print(f\"    Mean: {asset_returns.mean().values[0]:.6f}\")\n",
    "            print(f\"    Std Dev: {asset_returns.std().values[0]:.6f}\")\n",
    "            print(f\"    Min: {asset_returns.min().values[0]:.6f}\")\n",
    "            print(f\"    Max: {asset_returns.max().values[0]:.6f}\")\n",
    "else:\n",
    "    # For regular DataFrame\n",
    "    for col in market_returns.columns:\n",
    "        print(f\"  - {col}:\")\n",
    "        print(f\"    Mean: {market_returns[col].mean():.6f}\")\n",
    "        print(f\"    Std Dev: {market_returns[col].std():.6f}\")\n",
    "        print(f\"    Min: {market_returns[col].min():.6f}\")\n",
    "        print(f\"    Max: {market_returns[col].max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize return distributions\n",
    "def plot_return_distributions(returns, title=\"Return Distributions\"):\n",
    "    if returns.empty:\n",
    "        print(f\"Empty DataFrame, skipping {title} plot\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Handle MultiIndex DataFrame\n",
    "    if isinstance(returns.columns, pd.MultiIndex):\n",
    "        for asset in returns.columns.levels[1]:\n",
    "            asset_returns = returns.xs(asset, axis=1, level=1)\n",
    "            if not asset_returns.empty:\n",
    "                sns.kdeplot(asset_returns.iloc[:, 0], label=asset)\n",
    "    else:\n",
    "        # For regular DataFrame\n",
    "        for col in returns.columns:\n",
    "            sns.kdeplot(returns[col], label=col)\n",
    "    \n",
    "    plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Return\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot return distributions\n",
    "plot_return_distributions(market_returns, f\"{returns_method.capitalize()} Return Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load portfolio weights from configuration\n",
    "try:\n",
    "    with open(CONFIG_DIR / \"data_config.json\", 'r') as f:\n",
    "        data_config = json.load(f)\n",
    "    \n",
    "    # Extract weights from equity and bond indices\n",
    "    weights = {}\n",
    "    \n",
    "    for item in data_config[\"data_retrieval\"][\"equity_indices\"]:\n",
    "        weights[item[\"ticker\"]] = item[\"weight\"]\n",
    "    \n",
    "    for item in data_config[\"data_retrieval\"][\"bond_indices\"]:\n",
    "        weights[item[\"ticker\"]] = item[\"weight\"]\n",
    "    \n",
    "    print(\"Portfolio weights from configuration:\")\n",
    "    for asset, weight in weights.items():\n",
    "        print(f\"  - {asset}: {weight:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading portfolio weights: {e}\")\n",
    "    print(\"Using equal weights for all assets\")\n",
    "    weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before creating portfolio\n",
    "from src.data.retrieve_data import load_api_keys, get_yahoo_finance_data\n",
    "\n",
    "# If market data doesn't contain expected tickers, fetch them\n",
    "if not any(ticker in str(market_returns.columns) for ticker in ['SPX', 'US10YT=RR', '^GSPC', '^TNX']):\n",
    "    print(\"Market price data not found. Attempting to retrieve market data...\")\n",
    "    try:\n",
    "        # Get config data for tickers\n",
    "        with open(CONFIG_DIR / \"data_config.json\", 'r') as f:\n",
    "            data_config = json.load(f)\n",
    "        \n",
    "        config = data_config['data_retrieval']\n",
    "        equity_tickers = [item['ticker'] for item in config['equity_indices']]\n",
    "        bond_tickers = [item['ticker'] for item in config['bond_indices']]\n",
    "        \n",
    "        # Retrieve market data\n",
    "        market_price_data = get_yahoo_finance_data(\n",
    "            tickers=equity_tickers + bond_tickers,\n",
    "            start_date=market_returns.index[0].strftime('%Y-%m-%d'),\n",
    "            end_date=market_returns.index[-1].strftime('%Y-%m-%d')\n",
    "        )\n",
    "        \n",
    "        # Calculate returns\n",
    "        market_price_returns = calculate_returns(market_price_data, method=returns_method)\n",
    "        \n",
    "        # Now create portfolio from this data\n",
    "        portfolio_returns = create_portfolio(market_price_returns, weights=weights)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving market data: {e}\")\n",
    "        # Create dummy portfolio\n",
    "        portfolio_returns = pd.Series(index=market_returns.index, data=np.zeros(len(market_returns)))\n",
    "        print(\"Created dummy portfolio due to missing market data\")\n",
    "else:\n",
    "    # Standard portfolio creation\n",
    "    portfolio_returns = create_portfolio(market_returns, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we already have a valid portfolio from previous cell\n",
    "if 'portfolio_returns' in locals() and not (isinstance(portfolio_returns, pd.Series) and portfolio_returns.isna().all()):\n",
    "    print(\"Using previously created portfolio...\")\n",
    "else:\n",
    "    # Create portfolio\n",
    "    print(\"Creating portfolio from asset returns...\")\n",
    "    portfolio_returns = create_portfolio(market_returns, weights=weights)\n",
    "\n",
    "print(f\"Portfolio returns shape: {portfolio_returns.shape}\")\n",
    "\n",
    "# Display portfolio statistics\n",
    "print(\"\\nPortfolio statistics:\")\n",
    "print(f\"Mean return: {portfolio_returns.mean():.6f}\")\n",
    "print(f\"Standard deviation: {portfolio_returns.std():.6f}\")\n",
    "print(f\"Minimum return: {portfolio_returns.min():.6f}\")\n",
    "print(f\"Maximum return: {portfolio_returns.max():.6f}\")\n",
    "print(f\"Skewness: {portfolio_returns.skew():.4f}\")\n",
    "print(f\"Kurtosis: {portfolio_returns.kurtosis():.4f}\")\n",
    "print(f\"Sharpe ratio (assuming 0% risk-free rate): {portfolio_returns.mean() / portfolio_returns.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot portfolio returns\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot portfolio returns\n",
    "plt.subplot(2, 1, 1)\n",
    "portfolio_returns.plot()\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.title(\"Portfolio Returns Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot cumulative returns\n",
    "plt.subplot(2, 1, 2)\n",
    "cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "cumulative_returns.plot()\n",
    "plt.title(\"Cumulative Portfolio Returns\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot return distribution with normal distribution overlay\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot histogram of returns\n",
    "sns.histplot(portfolio_returns, kde=True, stat=\"density\", label=\"Actual Returns\")\n",
    "\n",
    "# Plot normal distribution with same mean and std\n",
    "x = np.linspace(portfolio_returns.min(), portfolio_returns.max(), 1000)\n",
    "y = stats.norm.pdf(x, portfolio_returns.mean(), portfolio_returns.std())\n",
    "plt.plot(x, y, 'r--', label=\"Normal Distribution\")\n",
    "\n",
    "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.title(\"Portfolio Return Distribution vs. Normal Distribution\")\n",
    "plt.xlabel(\"Return\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Q-Q plot to check normality\n",
    "plt.figure(figsize=(10, 10))\n",
    "stats.probplot(portfolio_returns, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Portfolio Returns\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Asset Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize asset correlations\n",
    "def calculate_correlations(returns):\n",
    "    if returns.empty:\n",
    "        print(\"Empty DataFrame, skipping correlation analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Handle MultiIndex DataFrame\n",
    "    if isinstance(returns.columns, pd.MultiIndex):\n",
    "        assets = returns.columns.levels[1]\n",
    "        corr_data = pd.DataFrame(index=assets, columns=assets)\n",
    "        \n",
    "        for asset1 in assets:\n",
    "            for asset2 in assets:\n",
    "                # Extract returns for the assets\n",
    "                asset1_returns = returns.xs(asset1, axis=1, level=1).iloc[:, 0]\n",
    "                asset2_returns = returns.xs(asset2, axis=1, level=1).iloc[:, 0]\n",
    "                \n",
    "                # Calculate correlation\n",
    "                correlation = asset1_returns.corr(asset2_returns)\n",
    "                corr_data.loc[asset1, asset2] = correlation\n",
    "    else:\n",
    "        # For regular DataFrame\n",
    "        corr_data = returns.corr()\n",
    "    \n",
    "    return corr_data\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = calculate_correlations(market_returns)\n",
    "\n",
    "if correlations is not None:\n",
    "    # Display correlation matrix\n",
    "    print(\"Asset Correlation Matrix:\")\n",
    "    display(correlations)\n",
    "    \n",
    "    # Visualize correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlations, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, fmt='.2f')\n",
    "    plt.title(\"Asset Return Correlations\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Relationship with Macroeconomic Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between portfolio returns and macro factors\n",
    "if not aligned_macro.empty and not portfolio_returns.empty:\n",
    "    # Align dates\n",
    "    common_dates = portfolio_returns.index.intersection(aligned_macro.index)\n",
    "    aligned_portfolio = portfolio_returns.loc[common_dates]\n",
    "    macro_subset = aligned_macro.loc[common_dates]\n",
    "    \n",
    "    if not common_dates.empty:\n",
    "        print(f\"Analyzing relationship with {len(macro_subset.columns)} macro factors over {len(common_dates)} dates\")\n",
    "        \n",
    "        # Calculate correlations\n",
    "        correlations = pd.DataFrame(index=macro_subset.columns, columns=['Correlation'])\n",
    "        \n",
    "        for factor in macro_subset.columns:\n",
    "            correlations.loc[factor, 'Correlation'] = aligned_portfolio.corr(macro_subset[factor])\n",
    "        \n",
    "        # Sort by absolute correlation\n",
    "        correlations['Abs_Correlation'] = correlations['Correlation'].abs()\n",
    "        correlations = correlations.sort_values(by='Abs_Correlation', ascending=False)\n",
    "        correlations = correlations.drop('Abs_Correlation', axis=1)\n",
    "        \n",
    "        # Display correlations\n",
    "        print(\"\\nPortfolio Return Correlations with Macro Factors:\")\n",
    "        display(correlations)\n",
    "        \n",
    "        # Plot top correlations\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        correlations['Correlation'].plot(kind='bar')\n",
    "        plt.title(\"Portfolio Return Correlations with Macro Factors\")\n",
    "        plt.ylabel(\"Correlation\")\n",
    "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot scatter plots for top factors\n",
    "        top_factors = correlations.index[:min(3, len(correlations))]\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, factor in enumerate(top_factors):\n",
    "            plt.subplot(1, 3, i+1)\n",
    "            plt.scatter(macro_subset[factor], aligned_portfolio, alpha=0.5)\n",
    "            plt.title(f\"{factor} vs. Portfolio Returns\")\n",
    "            plt.xlabel(factor)\n",
    "            plt.ylabel(\"Portfolio Return\")\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Add regression line\n",
    "            z = np.polyfit(macro_subset[factor], aligned_portfolio, 1)\n",
    "            p = np.poly1d(z)\n",
    "            plt.plot(macro_subset[factor], p(macro_subset[factor]), \"r--\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No common dates between portfolio returns and macro factors\")\n",
    "else:\n",
    "    print(\"Skipping macro factor analysis (no macro data available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data for Risk Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data for use in risk models\n",
    "def save_data(df, filename, directory=PROCESSED_DIR):\n",
    "    if df.empty:\n",
    "        print(f\"Empty DataFrame, skipping save for {filename}\")\n",
    "        return False\n",
    "    \n",
    "    # Make sure directory exists\n",
    "    directory.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    file_path = directory / filename\n",
    "    df.to_csv(file_path)\n",
    "    print(f\"Saved {len(df)} rows to {file_path}\")\n",
    "    return True\n",
    "\n",
    "# Save all processed data\n",
    "print(\"Saving processed data for risk models...\")\n",
    "save_data(aligned_market, \"market_data.csv\")\n",
    "save_data(aligned_macro, \"macro_data.csv\")\n",
    "save_data(market_returns, \"market_returns.csv\")\n",
    "save_data(portfolio_returns, \"portfolio_returns.csv\")\n",
    "save_data(correlations, \"asset_correlations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this notebook, we've performed comprehensive data cleaning and preprocessing for our risk management dashboard:\n",
    "\n",
    "1. Loaded and inspected raw market and macroeconomic data\n",
    "2. Identified and addressed data quality issues (missing values, outliers)\n",
    "3. Cleaned and aligned datasets to common dates\n",
    "4. Calculated returns using the configured method\n",
    "5. Created a portfolio based on asset weights\n",
    "6. Analyzed return distributions and correlations\n",
    "7. Examined relationships with macroeconomic factors\n",
    "8. Saved processed data for use in risk models\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- [Note: Add your key findings here based on the actual analysis results]\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Proceed to VaR modeling (`03_var_modeling.ipynb`)\n",
    "2. Implement Monte Carlo simulations (`04_monte_carlo_sim.ipynb`)\n",
    "3. Develop stress testing scenarios (`05_stress_testing.ipynb`)\n",
    "4. Validate models through backtesting (`06_backtesting.ipynb`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
